---
layout: page
title: About Us
permalink: /about/
---

We are the **Valley Team**, a group of researchers and engineers at ByteDance focused on multimodal large language models — models capable of understanding and generating content not only from text but also from images and video.

---

### What We Aim For

Our goal is to contribute something useful and reliable in the fast-evolving field of multimodal AI. We believe combining modalities can help make AI more helpful in real applications — like in short-video, e-commerce, content creation, and more.

---

### What We Do

- We build and improve models such as *Valley* and *Valley2*, which are designed to handle multimodal inputs (text, image, video) in a unified way.
- We work on new architectures and components — for example, *Valley2* introduces innovations like a large visual vocabulary, convolutional adapters, and an "Eagle Module" to better handle very long image/video inputs or extreme aspect ratios, while keeping training/inference cost reasonable.
- We collect and build datasets relevant to real use cases (e-commerce, short video, etc.), and we assemble benchmarks and instruction-tuning pipelines so that models don't just do well on academic tasks but also behave well "in the wild."
- We strive for openness: sharing code, model weights, datasets where possible, and encouraging feedback from the community.

---

### Recent Achievements (Humbly Speaking)

- *Valley2* achieved state-of-the-art performance on several e-commerce benchmarks, outperforming open-source models of similar size by a noteworthy margin.
- On the *OpenCompass* leaderboard for models with fewer than 10 billion parameters, *Valley2* ranks #2, with an average score around **67.4**.
- We've made progress in handling challenging visual inputs (very long image/video sequences, extreme aspect ratios) through model design improvements that help retain flexibility and efficiency.

---

### Our Values

- **Humility** — We are aware of how much there is still to learn. We expect to make mistakes, welcome critique, and continuously adjust.
- **Practicality** — Good performance on benchmarks is not enough; we care about robustness, efficiency, and usefulness in real scenarios.
- **Transparency** — We try to share what we can: code, findings, data (when respecting privacy, licensing, safety).
- **Collaboration** — We hope to learn from the community, and also to enable others to build on our work.

---

### Where We're Headed

- Extending to more modalities (e.g. audio), more complex forms of input (longer videos, more dynamic scenes).
- Improving reasoning capabilities, better chain-of-thought, more reliable instruction following.
- Making models more efficient in training and deployment, so they can be used more broadly.