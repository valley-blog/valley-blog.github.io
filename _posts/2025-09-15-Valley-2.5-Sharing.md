---
layout: post
title: Valley2.5 Sharing #1
tags: [Valley Pubilc, MLLM, Dataset]
---
# 简介
我们很高兴可以宣布释放 Valley 2.5，Valley 2.5 包含以下更新：

1. 模型结构更新：通过视觉编码器选型试验，我们更新了视觉编码器，采用了 Qwen2-VL 的视觉编码器（NaVit）
2. 数据集更新：我们更新了预训练和后训练数据集，采用一系列数据清洗方法，来保证数据的质量和多样性。
3. 训练方法更新：我们改进了训练方法，使用merge 的方式后训练（强化学习），增强了模型的多个方面能力
4. 评估方法更新：我们推出 miniBench 的评估集，大大缩短了评估时间，在该评估集能够获得与 fullBench 相当的评估效力。

# Model Arcture Update

## ViT 消融与选型：

我看到了团队对 Vision Transformer 的系统性评估，对比了 Qwen2.5VL-ViT, Qwen2VL-ViT, AIMv2-huge, InternViT-300M 等多种模型，并分析了固定分辨率（切图）与可变分辨率（NaViT）的优劣。

| **Vision Encoder**      | Params | **Resolution** | Model Link                                                   |
| ----------------------- | :----- | :------------- | :----------------------------------------------------------- |
| **Qwen2.5VL ViT**       | 600M   | NaViT          | [https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) |
| **Qwen2** **ViT**       | 600M   | NaViT          | [https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) |
| **MoonViT**             | 400M   | NaViT          | [https://huggingface.co/moonshotai/MoonViT-SO-400M](https://huggingface.co/moonshotai/MoonViT-SO-400M) |
| **Aimv2-huge**          | 600M   | 448p14         | [https://huggingface.co/apple/aimv2-huge-patch14-448](https://huggingface.co/apple/aimv2-huge-patch14-448) |
| **InternViT-300M-v2.5** | 300M   | 448p14         | [https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5](https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5) |
| **Oryx-ViT**            | 400M   | 384p16         | [https://huggingface.co/THUdyh/Oryx-ViT](https://huggingface.co/THUdyh/Oryx-ViT) |
| **SigLIP**              | 400M   | 384p14         | [https://huggingface.co/google/siglip-so400m-patch14-384](https://huggingface.co/google/siglip-so400m-patch14-384) |

- 评测方式：通过一阶段对齐实验（only tune projector），评测 s1 模型在多种能力 Bench 上的效果。
- 数据选择：对现有的 s1 阶段进行扩充，增加250w数据，最终 750w 数据。增加数据包括：
  - 开源视频 Caption 数据：Llava-video, Vript
  - 开源 OCR 数据：单词级OCR 和 长文本 OCR
  - 开源Grounding Caption数据 ：基于 Describe Anything 数据集构造
  - 开源多图数据：Internvl-sa-1b
- 评测 Bench：在 s1 原有的 Caption Bench 的基础上进行扩充。验证数据来自训练集采样

<div align="center">
  <img src="/images/2025-0915-valley-2.5-sharing/vit_result.png" alt="ViT 实验结果对比" width="80%">
  <p><em>图 1: 不同 Vision Encoder 在训练过程中的 Loss 变化对比</em></p>
</div>
<div class="info-box">
已经对 Qwen2.5vl-ViT、SigLip、Aimv2、Orxy-ViT 进行了消融实验：为提高实验速度，采用 Qwen3-1.7B 作为基座，projector 中间维度统一为 16k，并对 新 s1 数据采样 250w进行实验。 Loss 变化如下。其中，Qwen2.5-VL-ViT 的 Loss 明显更低data1.5
</div>

## Stage 2 Data
- data2

# Training Recipe

## MPO/DPO 对齐探索：
我跟踪了大量的 MPO (Multi-modal Policy Optimization) 实验，包括对数据规模（10k -> 100k -> 200k 的饱和现象）、学习率（1e-6 vs 2e-6 的敏感性）以及和 DPO 效果的对比。我还注意到了 DPO 与 SFT loss 比例（如 1:0.5）的 ablation study。

# Limited